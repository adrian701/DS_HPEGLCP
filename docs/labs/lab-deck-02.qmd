---
title: "Thinking in Data Science Workshop 03"
author: ""
date: "06-08-2022"
format:
  revealjs: 
    logo: img/00/logo_clear.png
    theme: moon
editor: visual
---

# Welcome back to our Thinking in Data Science Series!


## Reminders! {.smaller}

:::: {.columns}

::: {.column width="50%"}
Program website. 

> adrian701.github.io/DS_HPEGLCP/

<img src="img/01/website.png" style="display:block; margin: 0 auto; width: 92%">


:::

::: {.column width="50%"}
Program Wiki:

> [Thinkin in Data Science](https://rndwiki-pro.its.hpecorp.net/display/HCSS/Thinking+in+Data+Science+Workshop)

<img src="img/01/wiki.png" style="display:block; margin: 0 auto; width: 92%">

:::
>Next Workshop: Wednesday June 22nd, 2022 (bi-weekly)
<br>

Please join the Slack channel for questions and additional discussion:


#### \#glcs-data-science-series
::::

## Recap from last workshop {.smaller}

<img src="img/02/multi_level_categorical.png" style="display:block; margin: 0 auto; width: 100%">


:::: {.columns}

::: {.column width="50%"}
categorical features

<img src="img/02/boxplot_viz.png" style="display:block; margin: 0 auto; width: 125%">
:::

::: {.column width="50%"}
Advanced EDA

<img src="img/02/day_of_week.png" style="display:block; margin: 0 auto; width: 90%">
:::
::::

> Impact of visualizing our data and uncover meaningful relationships



# Any questions from last week's session?

## Workshop 3 Agenda (cont. DS Journey)


:::: {.columns}

::: {.column width="50%"}
EDA (Statistical Testing) 


<img src="img/00/data_science_process_eda.png" style="display:block; margin: 0 auto; width: 50%">

<img src="img/01/ANOVA_image.png" style="display:block; margin: 0 auto; width: 84%">
:::

::: {.column width="50%"}
Intro to Modeling

<img src="img/00/data_science_process_model.png" style="display:block; margin: 0 auto; width: 50%">

<img src="img/01/linear_regression.jpeg" style="display:block; margin: 0 auto; width: 60%">

:::

::::

# Statistics is centered on the idea of reasoning in an uncertain world, and reasoning from noisy data


## What is ANOVA?



- As a tool for data analysis, ANOVA is typically used to <span style="color:yellow">learn the relative importance</span> of different sources of variation in a dataset. 

:::{.element: class="fragment fade-up"}
- ANOVA test used to <span style="color:yellow">compare the means</span> of more than 2 groups (t-test can be used to compare 2 groups)
:::
:::{.element: class="fragment fade-up"}
- ANOVA uses variance-based F test to check the group mean equality. Sometimes, ANOVA F test is also called omnibus test as it tests non-specific null hypothesis i.e. all group means are equal
:::
<!-- :::{.element: class="fragment fade-up"} -->

<!-- - Main types: One-way (one factor) and two-way (two factors) ANOVA (factor is an independent variable) -->
<!-- ::: -->

# Primary use is a tool to test hypothesis and data analysis!


## What is a hypothesis and how do you set one up?

> Null hypothesis: Groups means are equal (no variation in means of groups)
H0: μ1=μ2=…=μp

>Alternative hypothesis: <span style="color:yellow">At least, one group mean is different</span> from other groups
H1: All μ are not equal

We use p-values and some other test statistics to <span style="color:yellow">determine if the effects are real and not due to random chance!</span>

More Reading on [hypothesis testing](https://www.reneshbedre.com/blog/hypothesis-testing.html) and more on Wiki Page

## Advantages of ANOVA {.smaller}

::: incremental
- Get <span style="color:yellow">better information on variables </span> (or treatments for experiments)
  - test statistical significance for added predictors in a linear model. 

- <span style="color:yellow">Compared effect </span> with the experimental error

- Looks at <span style="color:yellow">**interactions**</span> between variables
  - interaction means that a trend within one level of a variable is not parallel to a trend within another level of the same variable. 


<img src="img/01/anova_fun.png" style="display:block; margin: 0 auto; width: 25%">

:::


## Summary of calculating test statistics

- **df** means "the degrees of freedom in the source."
- **sum_sq** means "the sum of squares due to the source."
- **F** means "the F-statistic."
- **P** means "the P-value."

Rule of thumb: <span style="color:yellow">if p-value is less than .05, the relationship or mean difference we are observing is said to be statistically significant</span>

> see example in this week's notebook


## ANOVA Results Table  {.smaller}


<img src="img/01/anova_table2.png" style="display:block; margin: 0 auto; width: 20%">

Now, let's consider the row headings:

- **Factor** or **Between Groups** means ***the variability due to the factor of interest.***
- **Error** means ***the variability within the groups*** or ***unexplained random error.*** 
  - Consider this the <span style="color:yellow">remainder of the variation after accounting for the factor above</span>
- **Total** means ***the total variation in the data from the grand mean*** 


:::{.notes}
In the tire example on the previous page, the factor was the brand of the tire. In the learning example on the previous page, the factor was the method of learning.
Sometimes, the factor is a treatment, and therefore the row heading is instead labeled as Treatment. a 1. component that is due to the TREATMENT (or FACTOR), and
2. a component that is due to just RANDOM ERROR.
::::


## ANOVA and Regression libraries

```{.python code-line-numbers="|2|4|7|8"}
# import statistical test model functions
import statsmodels.api as sm
# import ordinary least square model
from statsmodels.formula.api import ols

# Ordinary Least Squares (OLS) model
model_part_day = ols('count ~ C(part_day)', data=bike_df).fit()
anova_table_part_day = sm.stats.anova_lm(model_part_day, typ=2)
anova_table_part_day

```


# Now the good stuff!

## Modeling 

<img src="img/02/ML_Algos.png" style="display:block; margin: 0 auto; width: 85%">



## Intro to Linear Regression


<img src="img/01/linear_regression.jpeg" style="display:block; margin: 0 auto; width: 80%">


## What is the goal of regression?

> A statistical approach that <span style="color:yellow">calculates the relative strengths </span> with which each input variable predisposes the outcome of interest.


:::{.element: class="fragment fade-up"}
> Uses maximum likelihood method for <span style="color:yellow">estimating the coefficients</span> of a statistical model
:::

:::{.notes}
MLE: method for estimating the parameters of a statistical model. One can think do MLE as a generative process for how the data you observe came to be. The idea of MLE is that the parameters which fit the data best are the parameters that give the highest probability to the data that you do observe

Assigning credits or weights to various touch points uncovers which channels or combinations of channels attributed the greatest impact/s on conversion. Attribution is used by advertisers to optimize marketing channels at a granular level and to support their understanding of their individual audiences and market as a whole.

Attribution lets you assign credit to each marketing channel and will allow you to observe how they contribute to conversions, for example online sales.
:::


:::{.element: class="fragment fade-up"}
> It is parametric model which means it has some <span style="color:yellow">statistical properties and assumptions</span> that must be satisfied
:::

## What questions can we answer with linear regression?


> In the context of marketing: What channel and sub-channel impression weights will generate the <span style="color:yellow"> maximum return </span> for a given line of business?

> In the context of home prices: Which home <span style="color:yellow">features will predict</span> the price of my home in the next 10 years?

> In the context of our bike-sharing demand: Which environmental and temporal features <span style="color:yellow">explain</span> the total demand for bike rentals?

## Advantages of regression 
::: incremental
- produces easily <span style="color:yellow">interpretable results</span>
  - as with any business decision, the ROI should be understood to the extent possible
- <span style="color:yellow">speed </span>
  - fastcomputing of model coefficients-aka feature weights
- <span style="color:yellow"> confidence intervals</span>
  - estimated feature weights come with confidence intervals



:::
:::{.notes}

interpretation
For example, in the medical field, it is not only important to predict the clinical outcome of a patient, but also to quantify the influence of the drug and at the same time take sex, age, and other features into account in an interpretable way.

compare regression to ML
linear regression has the advantage of produces easily interpretable results, but the accuracy of the predictions are likely to be lower than those achieved using a machine learning technique such as random forest.
By contrast, while random forest may give very accurate 
predictions it is challenging to interpret these 
predictions and understand the reason for which 
the prediction was made. (Ganes, 2018) 
Of course, as with any business decision the ROI 
should be understood to the extent possible. 

Confidence intervals
Estimated weights come with confidence intervals. A confidence interval is a range for the weight estimate that covers the “true” weight with a certain confidence. For example, a 95% confidence interval for a weight of 2 could range from 1 to 3. The interpretation of this interval would be: If we repeated the estimation 100 times with newly sampled data, the confidence interval would include the true weight in 95 out of 100 cases, given that the linear regression model is the correct model for the data.
:::

## Limitations of Regression

The drawbacks of linear models include:

::: incremental
- the inaccuracy of <span style="color:red">assuming linear relationships </span> 
  - not all features have a linear relation to the target variable. 
- the difficulty of using <span style="color:red">many inputs</span>
  - when features space is large, optimization for the coefficients is challenging
- <span style="color:red">multidimensional problems</span>
  - we run into collinearity issues (violation of model assumptions)
:::

## Model Formula Expression


<span style="color:white">$y=\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\epsilon$</span>

- linear regression model <span style="color:yellow">predicts</span> the target as a <span style="color:yellow">weighted sum </span> of the feature inputs
- the betas <span style="color:yellow">( $\beta_{j}$)</span> represent the learned feature weights/ coefficients. The first weight in the sum <span style="color:yellow">($\beta_0$) is called the intercept</span> and is not multiplied with a feature. 
- the epsilon <span style="color:yellow">( $\epsilon$) is the error</span> we still make, 
  - the difference between the prediction and the actual outcome.

## What is being optimized?

> Finding the value of the weights

<span style="color:white">$\hat{\boldsymbol{\beta}}=\arg\!\min_{\beta_0,\ldots,\beta_p}\sum_{i=1}^n\left(y^{(i)}-\left(\beta_0+\sum_{j=1}^p\beta_jx^{(i)}_{j}\right)\right)^{2}$</span>

- <span style="color:yellow"> ordinary least squares (OLS) method </span> is usually used to find the weights that minimize the squared differences between the actual and the estimated outcomes
  - finding the weight that gives us the smaller error rate possible

<!-- <img src="img/01/simple_linear_regression.png" style="display:block; margin: 0 auto; width: 60%"> -->

## Interpreting feature weights!

> Assuming statistical significance, the weight represents the average effect  (direction and magnitude) of the feature on the outcome of interest or prediction.


If the value of the feature weight is <span style="color:yellow">positive **(X)** </span>, one can say that an <span style="color:yellow">increase in **X** units increases the average outcome **Y** by **X** units.
</span>

In general, the higher the weight, the higher the effect of the weight is on the prediction, assuming the feature does not have a very low variance. We will need to construct an effect plot on the full model in order to determine which features are more influential (next workshop series).


:::{.notes}
notes on categorical feature
A solution to deal with many categories is the one-hot-encoding, meaning that each category has its own binary column. For a categorical feature with L categories, you only need L-1 columns, because the L-th column would have redundant information (e.g. when columns 1 to L-1 all have value 0 for one instance, we know that the categorical feature of this instance takes on category L).

Interpretation of a Numerical Feature

An increase of feature  x_k by one unit increases the prediction for y by  
β_k units when all other feature values remain fixed.

Interpretation of a Categorical Feature

Changing feature x_k rom the reference category to the other category increases the prediction for y by β_k when all other features remain fixed.


:::

## Interpreting a numerical features

<img src="img/02/numeric_interpretation.png" style="display:block; margin: 0 auto; width: 100%">



## Interpreting a binary features

<img src="img/02/binary_interpretation.png" style="display:block; margin: 0 auto; width: 100%">

## Interpreting a categorical features

### similar to binary features
<br>

> But what if we have, say 4 levels/unique category values? 
<br>

> Can you take a shot at explaining the feature "season" in our bike rental dataset?

:::{.element: class="fragment fade-up"}
<span style="color:white">$\#bike\:rentals=intercept+spring*(is\_spring)+\\+summer*(is\_summer)+fall*(is\_fall)\\+winter*(is\_winter)$</span>
:::

## Time for Lab!

<br>
<br>

### Open Thinking_in_Data_Science_03.ipynb on Google Colab from our github repo to get started

